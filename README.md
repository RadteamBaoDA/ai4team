# AI4Team - Production-Grade LLM Infrastructure with Security & Knowledge Base

A comprehensive, production-ready platform combining secure LLM deployment, knowledge base management, and enterprise infrastructure for AI-powered applications.

## ğŸ¯ Overview

**AI4Team** is a complete suite for deploying Large Language Models (LLMs) in enterprise environments with:

- ğŸ›¡ï¸ **Advanced Security**: LLM Guard integration with multilingual error handling
- ğŸ“š **Knowledge Base**: RAGFlow-powered document processing and retrieval
- ğŸ’¬ **Real-time Chat**: Load-balanced chat service with Nginx
- ğŸ“¦ **S3 Storage**: MinIO for artifact and model storage
- ğŸ³ **Container-Native**: Docker Compose for multi-environment deployment
- ğŸŒ **Multilingual**: Automatic language detection with localized responses
- âš™ï¸ **Enterprise Ready**: Nginx load balancing, rate limiting, SSL/TLS

## ğŸ“ Project Structure

```
ai4team/
â”œâ”€â”€ guardrails/              # LLM Security & Proxy Layer (PyPI-Ready Package)
â”‚   â”œâ”€â”€ src/ollama_guardrails/        # Modern src-layout package structure  
â”‚   â”œâ”€â”€ tests/                        # Test suite (unit & integration)
â”‚   â”œâ”€â”€ models/                       # Pre-trained model storage (9 models)
â”‚   â”œâ”€â”€ config/                       # Configuration files
â”‚   â”œâ”€â”€ docker/                       # Docker deployment configs
â”‚   â”œâ”€â”€ pyproject.toml                # Modern Python packaging (PEP 518/621)
â”‚   â”œâ”€â”€ README.md                     # ğŸ“– Detailed setup & usage guide
â”‚   â””â”€â”€ docs/                         # Comprehensive documentation
â”‚
â”œâ”€â”€ reranker/                # Document Reranking Service  
â”‚   â”œâ”€â”€ src/reranker/                 # Organized package structure
â”‚   â”œâ”€â”€ tests/                        # Test suite
â”‚   â”œâ”€â”€ config/                       # Environment configurations
â”‚   â”œâ”€â”€ pyproject.toml                # Modern Python packaging
â”‚   â””â”€â”€ README.md                     # ğŸ“– Setup & configuration guide
â”‚
â”œâ”€â”€ llm/                     # LiteLLM Proxy with Guard Integration
â”‚   â”œâ”€â”€ litellm_config.yaml           # LiteLLM configuration
â”‚   â”œâ”€â”€ docker-compose.yml            # Docker deployment
â”‚   â”œâ”€â”€ requirements.txt              # Python dependencies
â”‚   â””â”€â”€ README.md                     # ğŸ“– Deployment & integration guide
â”‚
â”œâ”€â”€ knowledgebase/           # RAGFlow Knowledge Base
â”‚   â”œâ”€â”€ docker-compose*.yml           # Multiple deployment configs
â”‚   â”œâ”€â”€ nginx/                        # Nginx proxy configuration
â”‚   â””â”€â”€ README.md                     # ğŸ“– Knowledge base setup guide
â”‚
â”œâ”€â”€ chat/                    # Real-time Chat Service
â”‚   â”œâ”€â”€ *.compose.yaml                # Master/slave deployment configs
â”‚   â””â”€â”€ README.md                     # ğŸ“– Chat service setup guide
â”‚
â”œâ”€â”€ s3/                      # MinIO S3-Compatible Storage
â”‚   â”œâ”€â”€ *.compose.yml                 # Master/slave deployment configs
â”‚   â””â”€â”€ README.md                     # ğŸ“– S3 storage setup guide
â”‚
â””â”€â”€ monitor/                 # Monitoring & Observability
    â”œâ”€â”€ langfuse.compose.yml          # Langfuse deployment
    â””â”€â”€ README.md                     # ğŸ“– Monitoring setup guide

### Local Utilities
- **structure_understand/** â€“ A lightweight CLI that walks a codebase, summarizes files with Ollama/OpenAI, and emits Markdown, JSON, and Tabler-styled HTML reports (complete with searchable tables, path filters, and raw JSON dumps). Run the tool with `uv run python structure_understand/app.py` or `PYTHONPATH=src python structure_understand/app.py` to ensure the new `src` layout is respected when you edit the modules in-tree. The HTML output now renders a Tabler `<table>`, exposes the serialized payload, and shows plain-text summaries for quick review while the CLI writes the same data to a configurable `json_output_file` for downstream tooling.
```

## ğŸš€ Quick Start

### Prerequisites

- Docker & Docker Compose
- Python 3.9-3.12 (for local development)
- 8GB+ RAM (recommended)
- NVIDIA GPU (optional, for faster inference)

### Quick Setup

Each service can be deployed independently. See individual README files for detailed instructions:

```bash
# 1. Guardrails (LLM Security Proxy) 
cd guardrails && pip install -e . && ollama-guardrails
# ğŸ“– See guardrails/README.md for full setup guide

# 2. Knowledge Base (RAGFlow)
cd knowledgebase && docker-compose up -d  
# ï¿½ See knowledgebase/README.md for configuration

# 3. Reranker Service
cd reranker && pip install -e . && reranker
# ğŸ“– See reranker/README.md for deployment options

# 4. LiteLLM Proxy  
cd llm && python run_litellm_proxy.py
# ğŸ“– See llm/README.md for integration guide

# 5. Chat Service
cd chat && docker-compose -f master.compose.yaml up -d
# ğŸ“– See chat/README.md for load balancing setup

# 6. S3 Storage (MinIO)
cd s3 && docker-compose -f master.compose.yml up -d
# ğŸ“– See s3/README.md for storage configuration

# 7. Monitoring (Langfuse)
cd monitor && docker-compose -f langfuse.compose.yml up -d
# ğŸ“– See monitor/README.md for observability setup
```

**ğŸ”— Component Documentation:**
- **[Guardrails](./guardrails/README.md)** - Security proxy with LLM Guard integration
- **[Reranker](./reranker/README.md)** - Document reranking with quantization support  
- **[LLM Proxy](./llm/README.md)** - Multi-provider LLM gateway with guardrails
- **[Knowledge Base](./knowledgebase/README.md)** - RAGFlow document processing
- **[Chat Service](./chat/README.md)** - Real-time chat with load balancing
- **[S3 Storage](./s3/README.md)** - MinIO distributed storage
- **[Monitoring](./monitor/README.md)** - Langfuse observability platform

## ğŸ” Core Features

### Security (Guardrails)
- **LLM Guard Integration**: 12 scanners (7 input + 5 output) for comprehensive security
- **Multilingual Support**: Error messages in 7 languages with automatic detection
- **Rate Limiting**: Nginx-based load balancing with SSL/TLS
- **ğŸ“– [Full Security Guide](./guardrails/README.md)**

### Knowledge Management 
- **RAGFlow Integration**: Document processing and semantic search
- **Multi-modal Support**: Text, images, PDFs, and more
- **ğŸ“– [Knowledge Base Guide](./knowledgebase/README.md)**

### Document Processing
- **Advanced Reranking**: Quantization support and smart batching
- **Multi-backend Support**: HuggingFace, MLX (Apple Silicon)
- **ğŸ“– [Reranker Service Guide](./reranker/README.md)**

### LLM Gateway
- **Multi-provider Support**: OpenAI, Anthropic, Azure, AWS Bedrock
- **Custom Guardrails**: Integrated security scanning
- **ğŸ“– [LLM Proxy Guide](./llm/README.md)**

### Infrastructure
- **Distributed Chat**: Master/slave load-balanced chat service
- **S3 Storage**: MinIO with automatic failover
- **Monitoring**: Langfuse observability and tracing
- **ğŸ“– [Infrastructure Guides](./chat/README.md) | [Storage](./s3/README.md) | [Monitoring](./monitor/README.md)**

## ğŸ”§ Configuration & Testing

Each service has its own configuration and testing setup. See individual README files for detailed information:

- **[Guardrails Configuration](./guardrails/README.md#configuration)** - YAML config, environment variables, security settings
- **[Reranker Configuration](./reranker/README.md#configuration)** - Environment configs for dev/prod/Apple Silicon
- **[LLM Proxy Configuration](./llm/README.md#configuration)** - Multi-provider setup and guardrail hooks
- **[Knowledge Base Configuration](./knowledgebase/README.md#configuration)** - Docker compose and service configs
- **[Infrastructure Configuration](./chat/README.md)** - Load balancing and distributed setup

### Quick Health Check

```bash
# Test all services
curl http://localhost:8080/health    # Guardrails
curl http://localhost:8001/health    # Reranker  
curl http://localhost:8000/health    # LLM Proxy
curl http://localhost:9380           # Knowledge Base
curl http://localhost:3000           # Monitoring
```

## ğŸ“– Documentation

Each service includes comprehensive documentation in its respective folder:

- **[ğŸ“– Guardrails](./guardrails/README.md)** - Setup, API reference, security configuration
- **[ğŸ“– Reranker](./reranker/README.md)** - Deployment, performance optimization, quantization  
- **[ğŸ“– LLM Proxy](./llm/README.md)** - Multi-provider setup, guardrail integration
- **[ğŸ“– Knowledge Base](./knowledgebase/README.md)** - RAGFlow configuration, document processing
- **[ğŸ“– Chat Service](./chat/README.md)** - Load balancing, master/slave setup
- **[ğŸ“– S3 Storage](./s3/README.md)** - MinIO deployment, distributed storage
- **[ğŸ“– Monitoring](./monitor/README.md)** - Langfuse observability, metrics

### Additional Documentation
- **[ğŸ“‹ Deployment Guides](./guardrails/docs/)** - Production deployment checklist
- **[ğŸ”§ Configuration Examples](./*/config/)** - Service-specific configuration files
- **[ğŸ§ª Testing Guides](./*/tests/)** - Unit and integration testing

## ğŸš¢ Deployment

### Quick Stack Deployment

```bash
# Deploy all services using Docker Compose
docker-compose -f guardrails/docker/docker-compose.yml \
               -f knowledgebase/docker-compose.yml \
               -f llm/docker-compose.yml \
               -f chat/master.compose.yaml \
               -f s3/master.compose.yml \
               -f monitor/langfuse.compose.yml up -d

# Or deploy services individually (see individual README files)
```

**ğŸ“– [Full Deployment Guide](./guardrails/docs/DEPLOYMENT.md)**

## ğŸ“ Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ¤ Support & Community

- ğŸ“– **Documentation**: See individual service README files for detailed guides
- ğŸ› **Issues**: [GitHub Issues](https://github.com/RadteamBaoDA/ai4team/issues)  
- ğŸ’¬ **Discussions**: [GitHub Discussions](https://github.com/RadteamBaoDA/ai4team/discussions)
- **Repository**: [github.com/RadteamBaoDA/ai4team](https://github.com/RadteamBaoDA/ai4team)

## ğŸ“ Learning Resources

- [Ollama](https://ollama.ai) | [LLM Guard](https://protectai.github.io/llm-guard/) | [RAGFlow](https://docs.ragflow.io)
- [FastAPI](https://fastapi.tiangolo.com/) | [Nginx](https://nginx.org/en/docs/)

## ğŸ“Š Project Stats

- **Total Endpoints**: 20+ (Ollama, OpenAI, Admin APIs)
- **Supported Languages**: 7 (Chinese, Vietnamese, Japanese, Korean, Russian, Arabic, English)
- **Security Scanners**: 12 (7 input + 5 output scanners)
- **Pre-trained Models**: 9 specialized models for security scanning
- **Core Services**: 6 (Guardrails, Reranker, LLM Proxy, Knowledge Base, Chat, S3)
- **Package Structure**: Modern src-layout with 5 organized subpackages
- **CLI Commands**: 2 entry points (ollama-guardrails, guardrails-server)
- **Source Files**: 120+ Python files across all services
- **Documentation Files**: 50+ comprehensive guides
- **Configuration Options**: 100+ settings across all services
- **Docker Compose Files**: 16 deployment configurations
- **Python Compatibility**: 3.9-3.12 with full type hints

## ğŸ—“ï¸ Version History

### v2.1.0 (Current - November 2025)
- âœ… **PyPI-Ready Package**: Complete src-layout structure with pyproject.toml
- âœ… **CLI Interface**: `ollama-guardrails` and `guardrails-server` commands
- âœ… **Modern Packaging**: PEP 518/621 compliant with optional dependencies
- âœ… **Model Organization**: Pre-configured model storage directories
- âœ… **Enhanced Testing**: Unit and integration test structure
- âœ… **Development Tools**: Black, isort, flake8, mypy configuration
- âœ… **Type Safety**: Full type hints with mypy validation
- âœ… **Package Structure**: Organized api/, core/, guards/, middleware/, utils/

### v2.0.0 (January 2025)
- âœ… **Major Architecture Refactor**: Modularized monolithic codebase
- âœ… **Enhanced Guardrails**: 12 scanners (7 input + 5 output)
- âœ… **Reranker Service**: Complete package with quantization support
- âœ… **LiteLLM Integration**: Custom guardrail hooks
- âœ… **Concurrency Control**: Per-model request management
- âœ… **Caching Layer**: Redis + in-memory fallback
- âœ… **Cross-platform Scripts**: Linux, macOS, Windows support
- âœ… **Modern Packaging**: pyproject.toml, structured modules

### v1.0.0 (December 2024)
- âœ… Complete LLM Guard integration
- âœ… Multilingual error messages (7 languages)
- âœ… Nginx load balancing with rate limiting
- âœ… Knowledge base integration (RAGFlow)
- âœ… S3 storage with MinIO
- âœ… Docker Compose deployment
- âœ… Comprehensive documentation

## ğŸ‰ Acknowledgments

Built with:
- [Ollama](https://ollama.ai) - LLM runtime
- [LLM Guard](https://protectai.github.io/llm-guard/) - Security scanning
- [RAGFlow](https://github.com/infiniflow/ragflow) - Knowledge base
- [FastAPI](https://fastapi.tiangolo.com/) - Web framework
- [Nginx](https://nginx.org/) - Load balancing
- [MinIO](https://min.io/) - S3 storage
- [Docker](https://docker.com/) - Containerization

---

**Last Updated**: November 1, 2025  
**Version**: v2.1.0 (PyPI-Ready Package with Modern Structure)  
**Maintained By**: [RadteamBaoDA](https://github.com/RadteamBaoDA)  
**License**: MIT
