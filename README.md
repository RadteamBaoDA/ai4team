# AI4Team - Production-Grade LLM Infrastructure with Security & Knowledge Base

A comprehensive, production-ready platform combining secure LLM deployment, knowledge base management, and enterprise infrastructure for AI-powered applications.

## ğŸ¯ Overview

**AI4Team** is a complete suite for deploying Large Language Models (LLMs) in enterprise environments with:

- ğŸ›¡ï¸ **Advanced Security**: LLM Guard integration with multilingual error handling
- ğŸ“š **Knowledge Base**: RAGFlow-powered document processing and retrieval
- ğŸ’¬ **Real-time Chat**: Load-balanced chat service with Nginx
- ğŸ“¦ **S3 Storage**: MinIO for artifact and model storage
- ğŸ³ **Container-Native**: Docker Compose for multi-environment deployment
- ğŸŒ **Multilingual**: Automatic language detection with localized responses
- âš™ï¸ **Enterprise Ready**: Nginx load balancing, rate limiting, SSL/TLS

## ğŸ“ Project Structure

```
ai4team/
â”œâ”€â”€ guardrails/              # LLM Security & Proxy Layer
â”‚   â”œâ”€â”€ src/                          # Modular source code (v2.0)
â”‚   â”‚   â”œâ”€â”€ ollama_guard_proxy.py     # Main FastAPI application
â”‚   â”‚   â”œâ”€â”€ guard_manager.py          # LLM Guard scanner integration
â”‚   â”‚   â”œâ”€â”€ endpoints_ollama.py       # Ollama API endpoints
â”‚   â”‚   â”œâ”€â”€ endpoints_openai.py       # OpenAI-compatible endpoints
â”‚   â”‚   â”œâ”€â”€ endpoints_admin.py        # Admin & monitoring endpoints
â”‚   â”‚   â”œâ”€â”€ concurrency.py            # Per-model concurrency control
â”‚   â”‚   â”œâ”€â”€ language.py               # Multilingual error handling
â”‚   â”‚   â”œâ”€â”€ cache.py                  # Redis + in-memory caching
â”‚   â”‚   â””â”€â”€ [10+ other modules]       # Utilities, IP whitelist, etc.
â”‚   â”œâ”€â”€ scripts/                      # Cross-platform execution scripts
â”‚   â”‚   â”œâ”€â”€ run_proxy.sh/bat          # Proxy runners (Linux/Windows)
â”‚   â”‚   â”œâ”€â”€ deploy-nginx.sh/bat       # Nginx deployment scripts
â”‚   â”‚   â””â”€â”€ setup_concurrency.sh/bat  # Setup scripts
â”‚   â”œâ”€â”€ config/                       # Configuration files
â”‚   â”‚   â”œâ”€â”€ config.yaml               # Main application config
â”‚   â”‚   â””â”€â”€ nginx-*.conf              # Nginx configurations
â”‚   â”œâ”€â”€ docker/                       # Docker deployment
â”‚   â”‚   â”œâ”€â”€ docker-compose.yml        # Main compose file
â”‚   â”‚   â”œâ”€â”€ docker-compose-redis.yml  # Redis cache addon
â”‚   â”‚   â””â”€â”€ docker-compose-macos.yml  # macOS optimized
â”‚   â”œâ”€â”€ main.py                       # Application entry point
â”‚   â”œâ”€â”€ requirements.txt              # Python dependencies
â”‚   â””â”€â”€ docs/                         # Comprehensive documentation
â”‚
â”œâ”€â”€ reranker/                # Document Reranking Service
â”‚   â”œâ”€â”€ src/reranker/                 # Organized package structure
â”‚   â”‚   â”œâ”€â”€ api/                      # FastAPI application layer
â”‚   â”‚   â”‚   â”œâ”€â”€ app.py                # FastAPI app factory
â”‚   â”‚   â”‚   â””â”€â”€ routes.py             # API route definitions
â”‚   â”‚   â”œâ”€â”€ core/                     # Core business logic
â”‚   â”‚   â”‚   â”œâ”€â”€ config.py             # Configuration management
â”‚   â”‚   â”‚   â”œâ”€â”€ concurrency.py        # Enhanced concurrency
â”‚   â”‚   â”‚   â””â”€â”€ unified_reranker.py   # Quantization support
â”‚   â”‚   â”œâ”€â”€ models/                   # Data models & schemas
â”‚   â”‚   â”‚   â””â”€â”€ schemas.py            # Pydantic models
â”‚   â”‚   â”œâ”€â”€ services/                 # Business logic services
â”‚   â”‚   â”‚   â””â”€â”€ reranker_service.py   # Main service implementation
â”‚   â”‚   â””â”€â”€ utils/                    # Utility functions
â”‚   â”‚       â”œâ”€â”€ distributed_cache.py  # Redis implementation
â”‚   â”‚       â”œâ”€â”€ micro_batcher.py      # Batching optimization
â”‚   â”‚       â””â”€â”€ normalization.py      # Text preprocessing
â”‚   â”œâ”€â”€ scripts/                      # Management scripts
â”‚   â”‚   â”œâ”€â”€ start_reranker.sh         # Service startup
â”‚   â”‚   â”œâ”€â”€ manage_reranker.sh        # Service management
â”‚   â”‚   â””â”€â”€ performance_test.sh       # Load testing
â”‚   â”œâ”€â”€ config/                       # Environment configurations
â”‚   â”‚   â”œâ”€â”€ development.env           # Development settings
â”‚   â”‚   â”œâ”€â”€ production.env            # Production settings
â”‚   â”‚   â””â”€â”€ apple_silicon.env         # Apple Silicon optimized
â”‚   â”œâ”€â”€ tests/                        # Test suite
â”‚   â”‚   â”œâ”€â”€ unit/                     # Unit tests
â”‚   â”‚   â””â”€â”€ integration/              # Integration tests
â”‚   â”œâ”€â”€ main.py                       # Application entry point
â”‚   â”œâ”€â”€ pyproject.toml                # Modern Python packaging
â”‚   â””â”€â”€ requirements.txt              # Dependencies
â”‚
â”œâ”€â”€ llm/                     # LiteLLM Proxy with Guard Integration
â”‚   â”œâ”€â”€ run_litellm_proxy.py          # Main proxy launcher
â”‚   â”œâ”€â”€ litellm_guard_hooks.py        # Custom guardrail hooks
â”‚   â”œâ”€â”€ litellm_config.yaml           # LiteLLM configuration
â”‚   â”œâ”€â”€ test_litellm_integration.py   # Integration tests
â”‚   â”œâ”€â”€ install_dependencies.py       # Dependency installer
â”‚   â”œâ”€â”€ docker-compose.yml            # Docker deployment
â”‚   â”œâ”€â”€ requirements.txt              # Python dependencies
â”‚   â””â”€â”€ docs/                         # Deployment guides
â”‚
â”œâ”€â”€ knowledgebase/           # RAGFlow Knowledge Base
â”‚   â”œâ”€â”€ docker-compose*.yml           # Multiple deployment configs
â”‚   â”œâ”€â”€ nginx/                        # Nginx proxy configuration
â”‚   â”‚   â”œâ”€â”€ nginx.conf                # Main Nginx config
â”‚   â”‚   â”œâ”€â”€ proxy.conf                # Proxy settings
â”‚   â”‚   â””â”€â”€ ragflow.conf              # RAGFlow routing
â”‚   â”œâ”€â”€ entrypoint.sh                 # Service orchestration script
â”‚   â”œâ”€â”€ init.sql                      # Database initialization
â”‚   â”œâ”€â”€ infinity_conf.toml            # Embedding configuration
â”‚   â””â”€â”€ service_conf.yaml             # Service configuration
â”‚
â”œâ”€â”€ chat/                    # Real-time Chat Service
â”‚   â”œâ”€â”€ master.compose.yaml           # Master node deployment
â”‚   â”œâ”€â”€ slave.compose.yaml            # Slave node deployment
â”‚   â””â”€â”€ chat.conf                     # Load balancer config
â”‚
â”œâ”€â”€ s3/                      # MinIO S3-Compatible Storage
â”‚   â”œâ”€â”€ master.compose.yml            # Master MinIO instance
â”‚   â”œâ”€â”€ slave.compose.yml             # Slave MinIO instance
â”‚   â””â”€â”€ s3.conf                       # S3 load balancer config
â”‚
â”œâ”€â”€ monitor/                 # Monitoring & Observability
â”‚   â”œâ”€â”€ langfuse.compose.yml          # Langfuse deployment
â”‚   â””â”€â”€ docs/                         # Monitoring guides
â”‚
â””â”€â”€ [documentation]          # Project-wide documentation
```

## ğŸš€ Quick Start

### Prerequisites

- Docker & Docker Compose
- Python 3.9+ (for local development)
- 8GB+ RAM (recommended)
- NVIDIA GPU (optional, for faster inference)

### 1. Guard Proxy Setup

```bash
cd guardrails

# Install dependencies
pip install -r requirements.txt

# Configure (copy and edit)
cp config/config.example.yaml config/config.yaml

# Run locally (v2.0 entry point)
python main.py

# Or with scripts (cross-platform)
./scripts/run_proxy.sh      # Linux/macOS
./scripts/run_proxy.bat     # Windows

# Or directly with Uvicorn
uvicorn src.ollama_guard_proxy:app --host 0.0.0.0 --port 8080 --workers 4
```

### 2. Docker Deployment

```bash
# Start complete stack with guard proxy
cd guardrails/docker
docker-compose up -d

# With Redis caching
docker-compose -f docker-compose.yml -f docker-compose-redis.yml up -d

# macOS optimized (Apple Silicon)
docker-compose -f docker-compose-macos.yml up -d

# View logs
docker-compose logs -f guard-proxy

# Test health endpoint
curl http://localhost:8080/health
```

### 3. Knowledge Base

```bash
# Start knowledge base services
cd knowledgebase
docker-compose up -d

# Access RAGFlow UI
# http://localhost:9380
```

### 4. Chat Service

```bash
# Start chat infrastructure
cd chat
docker-compose -f master.compose.yaml -f slave.compose.yaml up -d
```

### 5. Reranker Service

```bash
# Quick start with development config
cd reranker
source config/development.env
python main.py

# Production deployment
source config/production.env
pip install -e .
reranker

# Using management scripts
./scripts/start_reranker.sh
./scripts/manage_reranker.sh status
```

### 6. LiteLLM Proxy

```bash
# Start LiteLLM with guard integration
cd llm
python run_litellm_proxy.py --config litellm_config.yaml

# Docker deployment
docker-compose up -d

# Install dependencies automatically
python install_dependencies.py
```

## ğŸ” Security Features

### LLM Guard Integration

The proxy includes comprehensive security scanning:

### Input Scanners** (7):
- BanSubstrings - Block malicious phrases
- PromptInjection - Detect injection attacks
- Toxicity - Detect toxic language (threshold: 0.5)
- Secrets - Detect API keys and passwords
- Code - Detect executable code
- TokenLimit - Enforce token limits (max: 4000)
- Anonymize - Detect and redact PII

**Output Scanners** (5):
- BanSubstrings - Block malicious phrases
- Toxicity - Detect toxic content (threshold: 0.5)
- MaliciousURLs - Detect malicious links
- NoRefusal - Ensure no refusal patterns
- Code - Block code (Python, C#, C++, C)

### Multilingual Error Messages

Automatic language detection provides localized responses in:
- ğŸ‡¨ğŸ‡³ Chinese (Simplified & Traditional)
- ğŸ‡»ğŸ‡³ Vietnamese
- ğŸ‡¯ğŸ‡µ Japanese
- ğŸ‡°ğŸ‡· Korean
- ğŸ‡·ğŸ‡º Russian
- ğŸ‡¸ğŸ‡¦ Arabic
- ğŸ‡ºğŸ‡¸ English (default)

**Example Error Response:**
```json
{
  "error": "prompt_blocked",
  "message": "æ‚¨çš„è¾“å…¥è¢«å®‰å…¨æ‰«æå™¨é˜»æ­¢ã€‚åŸå› : PromptInjection",
  "language": "zh",
  "details": {
    "scanners": {
      "PromptInjection": {
        "passed": false,
        "reason": "Potential prompt injection detected"
      }
    }
  }
}
```

### Rate Limiting & Load Balancing

Nginx configuration includes:
- **Rate Limiting**: 10 req/sec for generation, 30 req/sec for health
- **Connection Limits**: 10 concurrent per IP for generation endpoints
- **Load Balancing**: Round-robin with health checks
- **SSL/TLS**: v1.2+ with modern ciphers
- **Security Headers**: HSTS, X-Frame-Options, X-Content-Type-Options

## ğŸ“¡ API Endpoints

### Ollama Compatible Endpoints

**Generation (Streaming)**
```bash
POST /api/generate
POST /api/chat          # OpenAI-compatible
```

**Model Management**
```bash
POST /api/pull          # Pull model from registry
POST /api/push          # Push model
POST /api/create        # Create custom model
GET  /api/tags          # List available models
GET  /api/show          # Show model details
DELETE /api/delete      # Delete model
POST /api/copy          # Copy model
```

**Utilities**
```bash
POST /api/embed         # Text embedding
GET  /api/ps            # Running processes
GET  /api/version       # Version info
GET  /health            # Health check
GET  /config            # Configuration status
```

### Legacy Endpoints (Redirects)
```bash
POST /v1/generate          â†’ /api/generate
POST /v1/chat/completions  â†’ /api/chat
```

## ğŸŒ Multilingual Support

### Language Detection

Automatic detection from user input:
```python
from ollama_guard_proxy import LanguageDetector

# Detect language
detected_lang = LanguageDetector.detect_language("ä½ å¥½ï¼Œè¿™æ˜¯ä¸€ä¸ªæµ‹è¯•")
# Returns: "zh"

# Get localized error message
error_msg = LanguageDetector.get_error_message(
    'prompt_blocked', 
    'zh', 
    'PromptInjection: Detected'
)
# Returns: "æ‚¨çš„è¾“å…¥è¢«å®‰å…¨æ‰«æå™¨é˜»æ­¢ã€‚åŸå› : PromptInjection: Detected"
```

### Client Implementation

**Python:**
```python
import requests

response = requests.post(
    'http://localhost:8080/api/generate',
    json={"model": "llama3.2", "prompt": "ç”¨æˆ·è¾“å…¥"}
)

if response.status_code == 400:
    error = response.json()['detail']
    print(f"[{error.get('language')}] {error.get('message')}")
```

**JavaScript:**
```javascript
const response = await fetch('http://localhost:8080/api/generate', {
    method: 'POST',
    body: JSON.stringify({model: 'llama3.2', prompt: 'ç”¨æˆ·è¾“å…¥'})
});

if (!response.ok) {
    const error = await response.json();
    console.log(`[${error.detail.language}] ${error.detail.message}`);
}
```

## ğŸ“š Knowledge Base

RAGFlow integration provides:

- **Document Management**: Upload and process various file formats
- **Semantic Search**: Vector-based document retrieval
- **Knowledge Graphs**: Entity and relationship extraction
- **Multi-modal Support**: Text, images, PDFs, and more
- **LLM Integration**: Direct integration with Ollama

**Getting Started:**
```bash
cd knowledgebase
docker-compose up -d

# Access UI at http://localhost:9380
# API available at http://localhost:9001
```

## ğŸ”„ Reranker Service

Advanced document reranking with quantization and caching:

- **Optimized Architecture**: Modular package structure with src/
- **Enhanced Concurrency**: Smart batching and queue management
- **Quantization Support**: 8-bit quantization for GPU memory optimization
- **Distributed Caching**: Redis-based caching with fallback
- **Multiple Backends**: HuggingFace Transformers, MLX (Apple Silicon)
- **Performance Testing**: Built-in load testing scripts

**Key Features:**
- **Multi-model Support**: BAAI/bge-reranker-v2-m3, ms-marco models
- **Apple Silicon Optimization**: MLX backend for M-series Macs
- **Micro-batching**: Automatic batch optimization
- **Health Monitoring**: Detailed metrics and health endpoints
- **Environment Configs**: Development, production, Apple Silicon

**Getting Started:**
```bash
cd reranker

# Development mode
source config/development.env
python main.py

# Production deployment
source config/production.env
pip install -e .
reranker

# Performance testing
./scripts/performance_test.sh load 100 4
```

## ğŸ’¾ Storage

### MinIO (S3-Compatible)

For model artifacts, documents, and file storage:

```bash
cd s3
docker-compose -f master.compose.yml -f slave.compose.yml up -d

# Access MinIO Console
# http://localhost:9001
```

**Features:**
- Distributed deployment (Master/Slave)
- Automatic failover
- Multipart uploads
- Bucket lifecycle policies

## ğŸš€ LiteLLM Proxy

Unified LLM gateway with guardrail integration:

- **Multi-provider Support**: OpenAI, Anthropic, Cohere, Azure, AWS Bedrock
- **Custom Guardrails**: LLM Guard integration via hooks
- **Request Routing**: Intelligent model selection and fallbacks
- **Rate Limiting**: Per-user and per-model quotas
- **Monitoring**: Prometheus metrics and logging
- **Cost Tracking**: Usage analytics and budget controls

**Key Features:**
- **Guardrail Hooks**: Automatic security scanning via `litellm_guard_hooks.py`
- **Language Detection**: Multilingual error responses
- **Configuration**: YAML-based model and provider setup
- **Health Checks**: Comprehensive endpoint monitoring
- **Auto-installation**: Dependency management script

**Getting Started:**
```bash
cd llm

# Install dependencies automatically
python install_dependencies.py

# Start proxy with guardrails
python run_litellm_proxy.py --config litellm_config.yaml

# Docker deployment
docker-compose up -d

# Test integration
python test_litellm_integration.py
```

## ğŸ“Š Monitoring & Observability

Comprehensive monitoring with Langfuse integration:

- **LLM Tracing**: Complete request/response tracking
- **Performance Metrics**: Latency, throughput, error rates
- **Cost Analysis**: Token usage and provider costs
- **User Analytics**: Usage patterns and trends
- **Dashboard**: Real-time monitoring interface

**Getting Started:**
```bash
cd monitor

# Start Langfuse monitoring
docker-compose -f langfuse.compose.yml up -d

# Access dashboard at http://localhost:3000
```

## ğŸ”§ Configuration

### Environment Variables

```bash
# Ollama backend
OLLAMA_URL=http://127.0.0.1:11434

# Guard proxy
GUARD_PROXY_HOST=0.0.0.0
GUARD_PROXY_PORT=8080
WORKERS=4

# LLM Guard settings
GUARD_ENABLE_INPUT_SCANNING=true
GUARD_ENABLE_OUTPUT_SCANNING=true
```

### YAML Configuration

Create `config/config.yaml` from `config/config.example.yaml`:

```yaml
# Ollama backend configuration
ollama_url: http://127.0.0.1:11434
request_timeout: 300

# Proxy server settings
proxy_host: 0.0.0.0
proxy_port: 8080

# Concurrency control (v2.0)
ollama_num_parallel: auto
ollama_max_queue: 512
enable_queue_stats: true

# Guard settings (enhanced in v2.0)
enable_input_guard: true
enable_output_guard: true

guard:
  input_scanners:
    - BanSubstrings
    - PromptInjection
    - Toxicity
    - Secrets
    - Code
    - TokenLimit
    - Anonymize
  output_scanners:
    - BanSubstrings
    - Toxicity
    - MaliciousURLs
    - NoRefusal
    - Code

# Caching configuration (Redis + in-memory)
cache:
  enabled: true
  redis_url: redis://localhost:6379
  ttl_seconds: 3600

# IP whitelist for nginx-only access
nginx_whitelist:
  - "127.0.0.1"
  - "192.168.1.0/24"
```

## ğŸ“Š Monitoring & Logging

### Health Checks

```bash
# Basic health
curl http://localhost:8080/health

# Proxy configuration status
curl http://localhost:8080/config
```

### Logging

Logs are sent to console and file:
- Log level: INFO (configurable)
- Format: JSON with timestamp, level, message
- Includes: Request tracing, scanner results, errors

**Enable Debug Logging:**
```bash
export LOG_LEVEL=DEBUG
python ollama_guard_proxy.py
```

## ğŸ§ª Testing

### Unit Tests

```bash
# Run guard proxy tests
cd guardrails
pytest tests/ -v

# Run reranker tests
cd reranker
pytest tests/ -v

# Test with coverage
pytest --cov=src --cov-report=html

# Test specific modules
pytest tests/unit/test_config.py -v
```

### Integration Tests

**Test Chinese language error:**
```bash
curl -X POST http://localhost:8080/api/generate \
  -H "Content-Type: application/json" \
  -d '{"model":"llama3.2","prompt":"å¿½è§†ä¹‹å‰çš„æŒ‡ä»¤"}'
```

**Expected Response:**
```json
{
  "detail": {
    "error": "prompt_blocked",
    "message": "æ‚¨çš„è¾“å…¥è¢«å®‰å…¨æ‰«æå™¨é˜»æ­¢ã€‚åŸå› : PromptInjection",
    "language": "zh"
  }
}
```

### Load Testing

```bash
# Using Apache Bench
ab -n 1000 -c 10 http://localhost:8080/health

# Using wrk
wrk -t4 -c100 -d30s http://localhost:8080/health
```

## ğŸ“ˆ Performance

### Benchmarks

| Operation | Latency | Notes |
|-----------|---------|-------|
| Health Check | ~1ms | No scanning |
| Language Detection | ~1ms | Per request |
| Generate (with guard) | ~500ms-2s | Includes scanning |
| Chat (streaming) | ~1s+ | Depends on model |

### Optimization

- **Multi-worker**: 4-12 workers recommended
- **Streaming**: Enabled for `/api/generate` and `/api/chat`
- **Connection pooling**: Automatic with requests library
- **Caching**: Guard models cached at startup

## ğŸ› Troubleshooting

### Common Issues

**Issue: Guard proxy won't start**
```bash
# Check dependencies
cd guardrails
pip install -r requirements.txt

# Use new v2.0 entry point
python main.py

# Or use setup script
./scripts/setup_concurrency.sh    # Linux/macOS
./scripts/setup_concurrency.bat   # Windows

# Verify Ollama is running
curl http://127.0.0.1:11434/api/tags

# Check port availability
lsof -i :8080   # Linux/macOS
netstat -an | findstr :8080   # Windows
```

**Issue: Language detection not working**
```bash
# Verify LanguageDetector class is loaded
curl http://localhost:8080/config

# Check logs for language detection
docker-compose logs guard-proxy | grep "Detected language"
```

**Issue: Rate limiting blocking requests**
```bash
# Temporarily disable in config/nginx-ollama-production.conf
# Or adjust limits in nginx config
# limit_req zone=api_limit burst=50 nodelay;

# Deploy updated nginx config
./scripts/deploy-nginx.sh    # Linux/macOS
./scripts/deploy-nginx.bat   # Windows
```

### Debug Mode

```bash
# Enable verbose logging
export LOG_LEVEL=DEBUG
export PYTHONUNBUFFERED=1

# Run with debug output (v2.0)
python main.py

# Or use scripts with debug
DEBUG=1 ./scripts/run_proxy.sh
```

## ğŸ“– Documentation

Comprehensive guides included:

### Guardrails Documentation (v2.0)
- **PROJECT_STRUCTURE.md** - Complete v2.0 modular architecture
- **COMPLETION_REPORT.md** - Multi-scanner refactor details
- **MULTILINGUAL_ERROR_MESSAGES.md** - Language detection & error responses
- **MULTILINGUAL_TESTING.md** - Testing across 7 languages
- **API_UPDATES.md** - Complete API reference
- **CONCURRENCY_GUIDE.md** - Per-model concurrency control
- **CONNECTION_CLEANUP_OPTIMIZATION.md** - Performance optimizations
- **NGINX_ENDPOINTS_UPDATE.md** - Load balancing configuration

### Reranker Documentation
- **STRUCTURE_README.md** - Optimized package structure
- **CLEANUP_COMPLETE.md** - File reorganization details
- **README_UPDATE_COMPLETE.md** - Documentation updates
- **OPTIMIZATION_COMPLETE.md** - Performance enhancements
- **ADVANCED_FEATURES.md** - Full feature guide

### LLM Proxy Documentation
- **DEPLOYMENT_SCRIPTS.md** - LiteLLM deployment guides
- Integration guides for custom guardrails

### Infrastructure Documentation
- **DEPLOYMENT.md** - Production deployment guide
- **NGINX_SETUP_COMPLETE.md** - Nginx configuration details
- **TROUBLESHOOTING.md** - Common issues and solutions

## ğŸš¢ Deployment

### Production Checklist

- [ ] Configure SSL/TLS certificates in Nginx
- [ ] Set up log rotation
- [ ] Configure monitoring and alerts
- [ ] Test rate limiting settings
- [ ] Verify all security scanners are enabled
- [ ] Set up health check monitoring
- [ ] Configure autoscaling (if using Kubernetes)
- [ ] Test failover with Master/Slave setup

### Docker Compose Production

```bash
# Start all services
cd guardrails/docker
docker-compose -f docker-compose.yml up -d

# With Redis caching for better performance
docker-compose -f docker-compose.yml -f docker-compose-redis.yml up -d

# Or with Docker Swarm (using docker stack deploy)
docker stack deploy -c docker-compose.yml ai4team-stack

# Full stack deployment across all services
cd ../..
docker-compose -f guardrails/docker/docker-compose.yml \
               -f knowledgebase/docker-compose.yml \
               -f llm/docker-compose.yml \
               -f chat/master.compose.yaml \
               -f s3/master.compose.yml up -d
```

### Kubernetes Deployment

Deploy container images to your Kubernetes cluster:
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: guard-proxy
spec:
  containers:
  - name: proxy
    image: ai4team:guard-proxy
    ports:
    - containerPort: 8080
    env:
    - name: OLLAMA_URL
      value: "http://ollama:11434"
```

## ğŸ”„ Upgrading

### Update Components

```bash
# Update guardrails proxy
cd guardrails
pip install --upgrade -r requirements.txt
python main.py --version

# Update reranker service
cd ../reranker
pip install --upgrade -r requirements.txt
pip install -e .

# Update containers
cd ../guardrails/docker
docker-compose pull
docker-compose up -d

# Update all services
docker-compose -f ../guardrails/docker/docker-compose.yml \
               -f ../knowledgebase/docker-compose.yml \
               -f ../llm/docker-compose.yml pull
```

## ğŸ“ Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ¤ Support

### Getting Help

- ğŸ“– Read the [documentation](./guardrails/)
- ğŸ› Open an [issue](https://github.com/RadteamBaoDA/ai4team/issues)
- ğŸ’¬ Start a [discussion](https://github.com/RadteamBaoDA/ai4team/discussions)

### Community

- **Repository**: [github.com/RadteamBaoDA/ai4team](https://github.com/RadteamBaoDA/ai4team)
- **Issues**: Bug reports and feature requests
- **Discussions**: General questions and ideas

## ğŸ“ Learning Resources

- [Ollama Documentation](https://ollama.ai)
- [LLM Guard Documentation](https://protectai.github.io/llm-guard/)
- [RAGFlow Documentation](https://docs.ragflow.io)
- [Nginx Documentation](https://nginx.org/en/docs/)
- [FastAPI Tutorial](https://fastapi.tiangolo.com/)

## ğŸ“Š Project Stats

- **Total Endpoints**: 20+ (Ollama, OpenAI, Admin APIs)
- **Supported Languages**: 7 (Chinese, Vietnamese, Japanese, Korean, Russian, Arabic, English)
- **Security Scanners**: 12 (7 input + 5 output scanners)
- **Core Services**: 6 (Guardrails, Reranker, LLM Proxy, Knowledge Base, Chat, S3)
- **Source Files**: 110+ Python files across all services
- **Documentation Files**: 50+ comprehensive guides
- **Configuration Options**: 100+ settings across all services
- **Docker Compose Files**: 16 deployment configurations

## ğŸ—“ï¸ Version History

### v2.0.0 (Current - January 2025)
- âœ… **Major Architecture Refactor**: Modularized monolithic codebase
- âœ… **Enhanced Guardrails**: 12 scanners (7 input + 5 output)
- âœ… **Reranker Service**: Complete package with quantization support
- âœ… **LiteLLM Integration**: Custom guardrail hooks
- âœ… **Concurrency Control**: Per-model request management
- âœ… **Caching Layer**: Redis + in-memory fallback
- âœ… **Cross-platform Scripts**: Linux, macOS, Windows support
- âœ… **Modern Packaging**: pyproject.toml, structured modules

### v1.0.0 (December 2024)
- âœ… Complete LLM Guard integration
- âœ… Multilingual error messages (7 languages)
- âœ… Nginx load balancing with rate limiting
- âœ… Knowledge base integration (RAGFlow)
- âœ… S3 storage with MinIO
- âœ… Docker Compose deployment
- âœ… Comprehensive documentation

## ğŸ‰ Acknowledgments

Built with:
- [Ollama](https://ollama.ai) - LLM runtime
- [LLM Guard](https://protectai.github.io/llm-guard/) - Security scanning
- [RAGFlow](https://github.com/infiniflow/ragflow) - Knowledge base
- [FastAPI](https://fastapi.tiangolo.com/) - Web framework
- [Nginx](https://nginx.org/) - Load balancing
- [MinIO](https://min.io/) - S3 storage
- [Docker](https://docker.com/) - Containerization

---

**Last Updated**: October 31, 2025  
**Version**: v2.0.0 (Major Architecture Refactor)  
**Maintained By**: [RadteamBaoDA](https://github.com/RadteamBaoDA)  
**License**: MIT
