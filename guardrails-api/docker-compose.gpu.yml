# LLM Guard API - GPU Docker Compose Configuration
# For systems with NVIDIA GPU support

services:
  llm-guard-api:
    build:
      context: .
      dockerfile: Dockerfile.gpu
    image: llm-guard-api:gpu
    container_name: llm-guard-api-gpu
    restart: unless-stopped
    ports:
      - "${APP_PORT:-8000}:8000"
    environment:
      # Application settings
      - APP_NAME=${APP_NAME:-LLM Guard API}
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - LOG_JSON=${LOG_JSON:-true}
      
      # Scan settings
      - SCAN_FAIL_FAST=${SCAN_FAIL_FAST:-true}
      - SCAN_PROMPT_TIMEOUT=${SCAN_PROMPT_TIMEOUT:-30}
      - SCAN_OUTPUT_TIMEOUT=${SCAN_OUTPUT_TIMEOUT:-60}
      - LAZY_LOAD=${LAZY_LOAD:-true}
      
      # Cache settings
      - CACHE_MAX_SIZE=${CACHE_MAX_SIZE:-10000}
      - CACHE_TTL=${CACHE_TTL:-3600}
      
      # Rate limiting
      - RATE_LIMIT_ENABLED=${RATE_LIMIT_ENABLED:-true}
      - RATE_LIMIT_LIMIT=${RATE_LIMIT_LIMIT:-60/minute}
      
      # Authentication (optional)
      - AUTH_TOKEN=${AUTH_TOKEN:-}
      
      # Observability
      - TRACING_EXPORTER=${TRACING_EXPORTER:-console}
      - METRICS_TYPE=${METRICS_TYPE:-prometheus}
    volumes:
      # Mount configuration
      - ./config/scanners.yml:/home/user/app/config/scanners.yml:ro
      # Mount models directory for local models (optional)
      - ./models:/home/user/app/models:ro
      # Logs directory
      - ./logs:/home/user/app/logs
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/healthz"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
    networks:
      - guardrails-network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

networks:
  guardrails-network:
    driver: bridge
    name: guardrails-network
