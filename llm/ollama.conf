# NGINX load balancer for two Ollama instances on different machines.
# Replace backend IPs/ports and server_name as needed.

worker_processes auto;
events { worker_connections 1024; }

http {
    # increase buffers for streaming/large payloads
    client_max_body_size 0;
    proxy_buffer_size 128k;
    proxy_buffers 4 256k;
    proxy_busy_buffers_size 256k;
    proxy_max_temp_file_size 0;

    upstream ollama_api {
        least_conn;
        server 192.168.1.2:11434 max_fails=3 fail_timeout=5s;
        server 192.168.1.11:11434 max_fails=3 fail_timeout=5s;
    }

    map $http_upgrade $connection_upgrade {
        default upgrade;
        ''      close;
    }

    server {
        listen 80;
        server_name ollama.ai4team.vn;  # change to your DNS / host

        # keep long streaming connections alive
        proxy_read_timeout 3600s;
        proxy_send_timeout 3600s;

        # API + streaming proxied to the same upstream
        location / {
            proxy_pass http://ollama_api;
            proxy_http_version 1.1;
            proxy_set_header Host localhost:11434;  # Ollama expects this Host header
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;

            # WebSocket/HTTP2 upgrade support (for streaming endpoints)
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection $connection_upgrade;

            proxy_buffering off;
            proxy_request_buffering off;
        }
    }
}