# Structure map for `D:\Project\ai4team\structure_understand\input`

_Generated from D:\Project\ai4team\structure_understand\config.yaml._

| Path | Type | Size | Summary |
| --- | --- | --- | --- |
| ./ | folder |  | Contains 2 subfolders |
| ./pptx | folder |  | Contains 1 subfolder |
| ./pptx/1 | folder |  | Contains 1 subfolder |
| ./pptx/1/2 | folder |  | Contains 1 subfolder |
| ./pptx/1/2/3 | folder |  | Contains 1 file |
| ./pptx/1/2/3/Performance_Testing_Presentation.pptx | file | 35398 | **Summary of “Performance_Testing_Presentation.pptx” (1/27/13)**    \| Section \| Key Points \| Supporting Details \| \|---------\|------------\|--------------------\| \| **Purpose** \| Demonstrate how to evaluate and improve the performance of a Windows desktop application and its underlying Web Service. \| • Focus on both client‑side (UI) and server‑side (API) performance. <br>• Emphasize the need for regular, repeatable tests after each release. \| \| **Client‑Side Performance Testing** \| • Measure UI responsiveness and load times. <br>• Identify bottlenecks in the desktop app before and after switching to a Web Service. \| • **Before Web Service** – load local data, profile with Visual Studio Profiler. <br>• **After Web Service** – call API to fetch settings/files, measure load time with Fiddler, logging, and profiler. <br>• Metrics: First Contentful Paint (FCP), Time to Interactive (TTI), DOM load time, JavaScript execution time. <br>• Tools: Google Lighthouse, WebPageTest, Chrome DevTools Performance panel. \| \| **Server‑Side Load Testing** \| • Test the Web API’s ability to handle concurrent clients. <br>• Capture average response time, error rate, and throughput. \| • **Tool** – Apache JMeter. <br>• **Test Plan** – Thread Group + HTTP Request sampler. <br>• **Scenario Example** – 500 requests/second for 1 minute to `/getFile`. <br>• **Results** – Avg = 780 ms, 95th = 1450 ms, Error Rate = 2 %. <br>• **Optimization Tips** – increase thread pool size, optimize I/O, enable GZIP compression. \| \| **Combined Approach** \| • Use client‑side metrics to improve user experience. <br>• Use load‑test metrics to define system limits. \| • Run both tests in parallel or sequentially to get a holistic view. <br>• Combine results to pinpoint whether UI delays are due to network/API latency or local processing. \| \| **Tools & References** \| • Google Lighthouse, JMeter, GTmetrix, Visual Studio Profiler, Fiddler, WebPageTest.org. \| • Official documentation links (not included in the slide but referenced). \| \| **Recommendations** \| • Execute tests regularly, especially after code or infrastructure changes. <br>• Document baseline metrics and track deviations. <br>• Address identified bottlenecks (e.g., increase pool size, enable compression). \| • Suggested to run tests in a staging environment that mirrors production. \| \| **Side Effects / Risks** \| • High error rates may indicate server overload or network issues. <br>• Poor client‑side performance can degrade perceived UX even if server metrics are fine. \| • Monitoring for spikes in latency or error counts is essential. \| \| **Expected Inputs for Tests** \| • API endpoint URLs, authentication tokens, request payloads. <br>• Desired concurrency level (threads/users), test duration, and request rate. \| • JMeter test plan configuration (Thread Group settings, HTTP Request sampler). \|  **Overall Message**   The presentation argues that effective performance testing requires a dual focus: measuring the end‑user experience on the client side and quantifying the server’s capacity to handle load. By combining tools such as Lighthouse for UI metrics and JMeter for API load testing, teams can identify bottlenecks, optimize system components, and maintain a high‑quality user experience. Regular, repeatable testing after each deployment is essential to ensure that performance regressions are caught early. |
| ./sample2xlxs | folder |  | Contains 1 file |
| ./sample2xlxs/Xay-dung-cau-hinh.xlsx | file | 12721 | **File Summary – sample2xlxs/Xay‑dung‑cau‑hinh.xlsx**  \| Section \| What it contains \| Key details \| \|---------\|------------------\|-------------\| \| **Price list (Bảng báo giá chi tiết)** \| A table that lists the cost of each component needed to build a PC. \| • **CPU** – Intel Core Ultra 7 (265 K / Turbo up to 5.5 GHz, 20 cores/20 threads, 30 MB cache, LGA 1851) – 8 590 000 ₫ (qty 1) <br>• **Motherboard** – ASUS PRIME Z890‑P WIFI CSM (DDR5) – 7 490 000 ₫ <br>• **RAM** – Corsair Vengeance DDR5 64 GB (2 × 32 GB, 6000 MHz) – 5 490 000 ₫ (qty 2) <br>• **GPU** – (price only) 29 990 000 ₫ <br>• **SSD** – 1 TB (price 2 990 000 ₫) <br>• **PSU** – ASUS TUF Gaming 1000 W, 80 Plus Gold, full‑modular – 4 750 000 ₫ <br>• **Case** – Coolermaster MasterBox MB520 TG ARGB – 2 150 000 ₫ <br>• **Cooling** – Deepcool LT720 AIO – 2 790 000 ₫ <br>• **Total** – 69 730 000 ₫ \|  \| Section \| What it contains \| Key details \| \|---------\|------------------\|-------------\| \| **Component list** \| A flat list of alternative parts that can be mixed and matched for different builds. \| • **CPU** – Intel i5‑14600KF, Intel Xeon E5‑2680 v4 (14 core/28 threads) <br>• **Motherboards** – MSI MPG B760M EDGE TI WIFI, ASUS PRIME Z890‑P, GIGABYTE GP‑AG4500G, MSI A750GF, Huanazhi X99 F8D Plus <br>• **RAM** – Corsair Dominator Platinum RGB 64 GB 5200 MHz, ECC DDR4 15 GB 2400, Galax 1060 6 GB <br>• **GPU** – MSI RTX 3070 Ti Gaming X Trio, ZOTAC GeForce RTX 4070 Ti SUPER Trinity OC, others <br>• **SSD** – Kingston NV3 1 TB M.2 PCIe Gen4 x4 NVMe, other models <br>• **PSU** – MIK SPOWER 650 W, other units <br>• **Cooling** – MAG CORELIQUID 240R V2, other AIO units \|  \| Section \| What it contains \| Key details \| \|---------\|------------------\|-------------\| \| **Numeric sequences** \| A series of numbers that appear to be cell coordinates or data mapping references. \| • Example: “0 1 2 3 4 5 6 7 8 9 8 10 11 12 9 12 13 14 15 16 17 18 42 19 9 19 20 41 21 9 21 23 24 25 9 25 26 27 28 9 28 29 30 31 9 31 22 32 33” <br>• These likely correspond to row/column indices used in the spreadsheet’s formulas or data validation rules. \|  ---  ### Purpose of the File The spreadsheet serves as a **configuration guide and cost calculator** for building personal computers. It provides:  1. **A detailed price breakdown** for a reference build (CPU, motherboard, RAM, GPU, SSD, PSU, case, cooling). 2. **A catalog of alternative components** that can be swapped in to create different builds (e.g., higher‑end GPUs, different CPUs, varied RAM capacities). 3. **Underlying data references** (numeric sequences) that support the spreadsheet’s calculations or data validation.  ### Main Message - **Transparency in cost**: Users can see exactly how each component contributes to the total price. - **Flexibility**: The component list allows users to tailor the build to their performance needs or budget constraints. - **Data integrity**: The numeric sequences suggest that the spreadsheet uses structured references to maintain consistency across calculations.  ### Supporting Details - All prices are listed in Vietnamese đồng (₫). - Quantities are specified for each item (e.g., 2 × 32 GB RAM modules). - The total cost is calculated automatically based on the unit prices and quantities.  ### Side Effects / Expected Inputs - **No explicit side effects** are noted; the file is purely informational. - **Expected inputs**: Users may need to adjust quantities or replace items from the component list to reflect their own build choices. The spreadsheet likely recalculates the total cost automatically when such changes are made.  ---  **Bottom line:**   The file is a practical tool for anyone planning to assemble a PC, offering a ready‑made cost estimate and a flexible set of component options. It is organized in Vietnamese, with clear sections for pricing, component alternatives, and underlying data references that support accurate calculations. |
