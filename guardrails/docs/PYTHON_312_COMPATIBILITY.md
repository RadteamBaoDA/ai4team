# Python 3.12 Compatibility Update

## Summary

This project has been updated to ensure **Python 3.12 compatibility** by removing PyTorch/torch dependencies and updating all packages to their latest Python 3.12 compatible versions.

## Changes Made

### 1. Dependencies Updated (`requirements.txt`)

#### Removed Packages:
- ‚ùå `torch==2.1.2` - Removed completely (GPU support no longer needed)
- ‚ùå `numpy==1.26.3` - Removed (was only needed for torch)

#### Updated Packages (Python 3.12 compatible):
- ‚úÖ `fastapi`: 0.109.0 ‚Üí 0.115.0
- ‚úÖ `uvicorn[standard]`: 0.27.0 ‚Üí 0.31.0
- ‚úÖ `pydantic`: 2.5.3 ‚Üí 2.9.2
- ‚úÖ `pydantic-settings`: 2.1.0 ‚Üí 2.5.2
- ‚úÖ `httpx`: 0.26.0 ‚Üí 0.27.2
- ‚úÖ `pyyaml`: 6.0.1 ‚Üí 6.0.2
- ‚úÖ `orjson`: 3.9.12 ‚Üí 3.10.7
- ‚úÖ `uvloop`: 0.19.0 ‚Üí 0.20.0
- ‚úÖ `cachetools`: 5.3.2 ‚Üí 5.5.0
- ‚úÖ `redis[async]`: 5.0.1 ‚Üí 5.1.1
- ‚úÖ `hiredis`: 2.3.2 ‚Üí 3.0.0
- ‚úÖ `cryptography`: 42.0.0 ‚Üí 43.0.1
- ‚úÖ `prometheus-client`: 0.19.0 ‚Üí 0.21.0
- ‚úÖ `psutil`: 5.9.7 ‚Üí 6.1.0

#### Retained Packages:
- ‚úÖ `llm-guard==0.3.16` - Core security library (unchanged)

### 2. Code Changes

#### `performance.py`
- Removed torch imports and GPU monitoring code
- Updated `_detect_platform()` to remove PyTorch version detection
- Updated `get_gpu_metrics()` to return disabled status with explanation
- CPU monitoring remains fully functional

#### `guard_manager.py`
- Removed torch imports
- Updated `_detect_device()` to always return 'cpu' (no GPU support)
- Simplified `_configure_local_models()` to CPU-only configuration
- Removed MPS (Apple Silicon GPU) and CUDA-specific optimizations
- All ML inference now runs on CPU through llm-guard

#### `run_proxy_macos.sh`
- Renamed `check_mps_support()` to `check_device_support()`
- Removed MPS/PyTorch availability checks
- Removed `PYTORCH_MPS_HIGH_WATERMARK_RATIO` environment variable
- Updated messaging to reflect CPU-only operation

### 3. Impact Assessment

#### What Still Works ‚úÖ
- ‚úÖ All security scanners (llm-guard) - run on CPU
- ‚úÖ Input/output scanning and validation
- ‚úÖ PII anonymization and secrets detection
- ‚úÖ Redis caching and performance monitoring
- ‚úÖ All API endpoints and proxy functionality
- ‚úÖ FastAPI and Uvicorn web server
- ‚úÖ Multi-worker concurrency

#### What Changed ‚ö†Ô∏è
- ‚ö†Ô∏è **GPU acceleration disabled**: All ML models now run on CPU
- ‚ö†Ô∏è **Apple Silicon MPS not used**: M1/M2/M3 Macs use CPU instead of Metal GPU
- ‚ö†Ô∏è **CUDA not used**: NVIDIA GPU acceleration unavailable
- ‚ö†Ô∏è **Performance impact**: Slightly slower ML inference (CPU vs GPU)

#### Performance Notes üìä
- CPU-based inference is still fast for most use cases
- Apple Silicon CPUs (M1/M2/M3) have excellent performance even without GPU
- For high-throughput scenarios, consider:
  - Increasing worker count
  - Using Redis for caching results
  - Optimizing scanner configuration

## Installation

### Requirements
- Python 3.12 or higher
- No GPU/CUDA requirements
- Works on all platforms (macOS, Linux, Windows)

### Setup
```bash
# Create virtual environment with Python 3.12
python3.12 -m venv venv

# Activate virtual environment
source venv/bin/activate  # On macOS/Linux
# or
venv\Scripts\activate  # On Windows

# Install dependencies
pip install -r requirements.txt
```

### Running the Proxy
```bash
# Start in background
./run_proxy.sh start

# Run in foreground (development)
./run_proxy.sh run

# macOS specific script (still works, CPU-only)
./run_proxy_macos.sh start
```

## Environment Variables

### Device Configuration (Updated)
```bash
# Device selection (only 'cpu' supported now)
export LLM_GUARD_DEVICE="cpu"  # Default and only option

# GPU-related variables (no longer used, kept for compatibility)
# export PYTORCH_ENABLE_MPS_FALLBACK="1"  # Ignored
# export MPS_ENABLE_FP16="true"           # Ignored
```

### Other Configuration (Unchanged)
```bash
# Local model configuration
export LLM_GUARD_USE_LOCAL_MODELS="true"
export LLM_GUARD_MODELS_PATH="./models"

# Redis caching
export REDIS_ENABLED="true"
export REDIS_HOST="localhost"
export REDIS_PORT="6379"

# Proxy settings
export PROXY_PORT="9999"
export OLLAMA_URL="http://127.0.0.1:11434"
```

## Migration Guide

### For Existing Deployments

1. **Update Python**: Ensure you're using Python 3.12+
   ```bash
   python --version  # Should show 3.12.x
   ```

2. **Recreate Virtual Environment**:
   ```bash
   rm -rf venv
   python3.12 -m venv venv
   source venv/bin/activate
   ```

3. **Install Updated Dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

4. **Remove GPU-Related Environment Variables** (optional):
   - Remove or ignore `PYTORCH_*` and `MPS_*` variables
   - Set `LLM_GUARD_DEVICE="cpu"` (default)

5. **Restart the Proxy**:
   ```bash
   ./run_proxy.sh restart
   ```

### For Docker Deployments

Update your Dockerfile to use Python 3.12 base image:
```dockerfile
FROM python:3.12-slim

# Rest of your Dockerfile...
COPY requirements.txt .
RUN pip install -r requirements.txt
```

## Testing

After upgrading, verify functionality:

```bash
# Check health endpoint
curl http://localhost:9999/health

# Test input scanning
curl -X POST http://localhost:9999/api/generate \
  -H "Content-Type: application/json" \
  -d '{"model":"mistral","prompt":"Hello world"}'

# Check metrics
curl http://localhost:9999/metrics
```

## Benefits of This Update

1. ‚úÖ **Python 3.12 Compatible**: Latest Python features and security updates
2. ‚úÖ **Simplified Dependencies**: Fewer packages to manage
3. ‚úÖ **Smaller Installation**: No large torch/numpy packages (~1-2GB saved)
4. ‚úÖ **Faster Install Time**: Reduced dependency installation time
5. ‚úÖ **Better Compatibility**: Works consistently across all platforms
6. ‚úÖ **Easier Deployment**: No CUDA/GPU driver requirements

## Future Considerations

If GPU acceleration is needed in the future:
- Consider using ONNX Runtime (lighter than torch)
- Evaluate llm-guard's built-in optimization options
- Use hardware-accelerated inference servers (e.g., Triton)
- Scale horizontally with more CPU workers

## Support

For issues or questions:
1. Check logs: `tail -f guardrails/proxy.log`
2. Verify Python version: `python --version`
3. Check installed packages: `pip list`
4. Test dependencies: `python -c "import llm_guard; print('OK')"`

---

**Last Updated**: October 31, 2025
**Python Version**: 3.12+
**Status**: Production Ready ‚úÖ
