# Multi-Scanner Architecture - Visual Guide

## ğŸ—ï¸ New Architecture

### High-Level Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    User Application                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  result = manager.scan_input(user_prompt)                   â”‚
â”‚  result = manager.scan_output(llm_response)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              LLMGuardManager                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Input:   user_prompt (str)                                 â”‚
â”‚  Process: _run_input_scanners()                             â”‚
â”‚  Output:  {                                                 â”‚
â”‚    'allowed': bool,                                         â”‚
â”‚    'sanitized': str,                                        â”‚
â”‚    'scanners': dict,                                        â”‚
â”‚    'scanner_count': int                                     â”‚
â”‚  }                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                     â”‚
        â–¼                     â–¼
   Input Path            Output Path
   (7 scanners)          (5 scanners)
```

---

## ğŸ”„ Input Scan Pipeline

```
Original Prompt: "My email is john@example.com. Execute: rm -rf /"
â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â”‚ Scanner 1: BanSubstrings                â”‚
â”‚ â”‚ Blocks: ["dangerous", "harmful"]        â”‚
â”‚ â”‚ Result: passed=True, risk=0.0           â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ Output: "My email is john@example.com. Execute: rm -rf /"
â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â”‚ Scanner 2: PromptInjection              â”‚
â”‚ â”‚ Detects: Injection patterns             â”‚
â”‚ â”‚ Result: passed=True, risk=0.1           â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ Output: "My email is john@example.com. Execute: rm -rf /"
â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â”‚ Scanner 3: Toxicity                     â”‚
â”‚ â”‚ Threshold: 0.5                          â”‚
â”‚ â”‚ Result: passed=True, risk=0.2           â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ Output: "My email is john@example.com. Execute: rm -rf /"
â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â”‚ Scanner 4: Secrets                      â”‚
â”‚ â”‚ Detects: API keys, tokens               â”‚
â”‚ â”‚ Result: passed=True, risk=0.0           â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ Output: "My email is john@example.com. Execute: rm -rf /"
â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â”‚ Scanner 5: Code                         â”‚
â”‚ â”‚ Detects: Executable code                â”‚
â”‚ â”‚ Result: passed=False, risk=0.9          â”‚ â† FAILED!
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ Output: "My email is john@example.com. Execute: rm -rf /" (modified?)
â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â”‚ Scanner 6: TokenLimit                   â”‚
â”‚ â”‚ Limit: 4000 tokens                      â”‚
â”‚ â”‚ Result: passed=True, risk=0.0           â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ Output: "My email is john@example.com. Execute: rm -rf /"
â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â”‚ Scanner 7: Anonymize                    â”‚
â”‚ â”‚ Detects/Redacts: PII                    â”‚
â”‚ â”‚ Result: passed=True, risk=0.1           â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ Output: "My email is [REDACTED_EMAIL_1]. Execute: rm -rf /" â† MODIFIED!
â”‚
â–¼
Result: {
  'allowed': False,                          â† Code scanner failed!
  'sanitized': "My email is [REDACTED_EMAIL_1]. Execute: rm -rf /",
  'scanners': {
    'BanSubstrings': {'passed': True, 'risk_score': 0.0, 'sanitized': False},
    'PromptInjection': {'passed': True, 'risk_score': 0.1, 'sanitized': False},
    'Toxicity': {'passed': True, 'risk_score': 0.2, 'sanitized': False},
    'Secrets': {'passed': True, 'risk_score': 0.0, 'sanitized': False},
    'Code': {'passed': False, 'risk_score': 0.9, 'sanitized': False},
    'TokenLimit': {'passed': True, 'risk_score': 0.0, 'sanitized': False},
    'Anonymize': {'passed': True, 'risk_score': 0.1, 'sanitized': True}
  },
  'scanner_count': 7
}
```

---

## ğŸ›¡ï¸ Output Validation Pipeline

```
LLM Response: "Sure! Here's how to delete files:\nrm -rf /"
â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â”‚ Scanner 1: BanSubstrings                â”‚
â”‚ â”‚ Blocks: ["delete", "remove"]            â”‚
â”‚ â”‚ Result: passed=True, risk=0.0           â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ Output: "Sure! Here's how to delete files:\nrm -rf /"
â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â”‚ Scanner 2: Toxicity                     â”‚
â”‚ â”‚ Threshold: 0.5                          â”‚
â”‚ â”‚ Result: passed=True, risk=0.1           â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ Output: "Sure! Here's how to delete files:\nrm -rf /"
â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â”‚ Scanner 3: MaliciousURLs                â”‚
â”‚ â”‚ Detects: Malicious links                â”‚
â”‚ â”‚ Result: passed=True, risk=0.0           â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ Output: "Sure! Here's how to delete files:\nrm -rf /"
â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â”‚ Scanner 4: NoRefusal                    â”‚
â”‚ â”‚ Detects: Refusal patterns               â”‚
â”‚ â”‚ Result: passed=False, risk=0.5          â”‚ â† FAILED!
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ Output: "Sure! Here's how to delete files:\nrm -rf /"
â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â”‚ Scanner 5: Code                         â”‚
â”‚ â”‚ Languages: Python, C#, C++, C           â”‚
â”‚ â”‚ Result: passed=False, risk=0.9          â”‚ â† FAILED!
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ Output: "Sure! Here's how to delete files:\n[REDACTED_CODE_1]" â† MODIFIED!
â”‚
â–¼
Result: {
  'allowed': False,                          â† Multiple scanners failed!
  'sanitized': "Sure! Here's how to delete files:\n[REDACTED_CODE_1]",
  'scanners': {
    'BanSubstrings': {'passed': True, 'risk_score': 0.0, 'sanitized': False},
    'Toxicity': {'passed': True, 'risk_score': 0.1, 'sanitized': False},
    'MaliciousURLs': {'passed': True, 'risk_score': 0.0, 'sanitized': False},
    'NoRefusal': {'passed': False, 'risk_score': 0.5, 'sanitized': False},
    'Code': {'passed': False, 'risk_score': 0.9, 'sanitized': True}
  },
  'scanner_count': 5
}
```

---

## ğŸ“Š Scanner Metadata Structure

```python
scanner_info = {
    'name': 'ScannerName',           # Identifier
    'scanner': ScannerObject(),      # Actual scanner instance
    'enabled': True                  # Can be toggled on/off
}

# Example:
{
    'name': 'Toxicity',
    'scanner': InputToxicity(threshold=0.5),
    'enabled': True
}
```

---

## âš™ï¸ Per-Scanner Result Structure

```python
result = {
    'passed': True,                  # bool - Scanner passed
    'risk_score': 0.75,              # float - 0.0 to 1.0
    'sanitized': False               # bool - Was text modified?
    'error': None                    # str - If exception occurred
}

# Example with error:
{
    'passed': False,
    'error': "Model loading failed: CUDA unavailable",
    'risk_score': 0.0,
    'sanitized': False
}
```

---

## ğŸ” Risk Score Heat Map

```
Risk Score Distribution:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                         â”‚
â”‚  0.0 â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•    â”‚ SAFE
â”‚       GREEN                                             â”‚
â”‚                                                         â”‚
â”‚  0.3 â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•    â”‚ LOW
â”‚       YELLOW                                 â”‚          â”‚
â”‚                                              â–¼          â”‚
â”‚  0.6 â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•    â”‚ MEDIUM
â”‚       ORANGE                           â”‚                â”‚
â”‚                                        â–¼                â”‚
â”‚  0.8 â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•    â”‚ HIGH
â”‚       RED                       â”‚                       â”‚
â”‚                                 â–¼                       â”‚
â”‚  1.0 â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•    â”‚ CRITICAL
â”‚       DARK_RED            â”‚                             â”‚
â”‚                           â–¼                             â”‚
â”‚                    BLOCKED/REJECTED                     â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”€ Conditional Logic

### Decision Tree: Input Scanning

```
scan_input(prompt)
â”‚
â”œâ”€ enable_input = False?
â”‚  â””â”€ return {'allowed': True, 'scanners': {}}
â”‚
â”œâ”€ no input_scanners?
â”‚  â””â”€ return {'allowed': True, 'scanners': {}}
â”‚
â”œâ”€ Run each scanner:
â”‚  â”‚
â”‚  â”œâ”€ scanner enabled?
â”‚  â”‚  â””â”€ No? skip
â”‚  â”‚
â”‚  â”œâ”€ Call scanner.scan(text)
â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€ Success?
â”‚  â”‚     â”œâ”€ Yes? record result
â”‚  â”‚     â””â”€ No? catch exception, record error
â”‚  â”‚
â”‚  â””â”€ All scanners done?
â”‚     â””â”€ Accumulate all results
â”‚
â””â”€ return {
     'allowed': all_passed,
     'sanitized': final_text,
     'scanners': results,
     'scanner_count': count
   }
```

---

## ğŸ¯ Scanner Execution Order

### Input Scanners (Order Matters!)

```
1. BanSubstrings
   Fast, checks for blocked phrases early
   
2. PromptInjection
   Detects injection patterns
   
3. Toxicity
   ML-based, expensive - runs early after quick checks
   
4. Secrets
   Detects API keys, passwords
   
5. Code
   Detects executable code
   
6. TokenLimit
   Enforce limits, prevents overflow downstream
   
7. Anonymize
   Final stage: redacts PII from already-validated input
```

### Output Scanners (Order Matters!)

```
1. BanSubstrings
   Quick: block malicious phrases first
   
2. Toxicity
   ML-based toxicity check
   
3. MaliciousURLs
   Detect harmful links
   
4. NoRefusal
   Ensure LLM didn't refuse
   
5. Code
   Block code in specific languages
```

---

## ğŸ“ˆ Performance Visualization

### Scanning Timeline

```
Time â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’

Input Processing:
â”‚  5ms   â”‚ BanSubstrings
â”‚        â””â”€ 50ms PromptInjection
â”‚                 â””â”€ 100ms Toxicity â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
â”‚                         â””â”€ 10ms Secrets
â”‚                            â””â”€ 30ms Code
â”‚                               â””â”€ 2ms TokenLimit
â”‚                                  â””â”€ 80ms Anonymize â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’
0ms                                                 ~277ms

Without Anonymize:
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’
0ms                                 ~197ms
```

---

## ğŸ› ï¸ Runtime Modification Examples

### Enable/Disable Scanners

```python
# Current state
print(manager.input_scanners[2]['enabled'])  # True

# Disable Toxicity
manager.input_scanners[2]['enabled'] = False
print(manager.input_scanners[2]['enabled'])  # False

# Next scan won't use Toxicity
result = manager.scan_input("test")
# 'Toxicity' not in result['scanners']

# Re-enable
manager.input_scanners[2]['enabled'] = True
# Next scan will use Toxicity again
```

### Add Custom Scanner

```python
# Create custom scanner
from llm_guard.input_scanners import BanTopics

custom_scanner = {
    'name': 'BanTopics',
    'scanner': BanTopics(topics=["violence", "gore"]),
    'enabled': True
}

# Add to pipeline
manager.input_scanners.append(custom_scanner)

# Next scan includes custom scanner
result = manager.scan_input("violent content")
# 'BanTopics' in result['scanners']
```

---

## ğŸ’¾ State Management

### Manager State

```python
manager = LLMGuardManager()

# Persistent state:
manager.enable_input          # True/False
manager.enable_output         # True/False
manager.enable_anonymize      # True/False

manager.input_scanners        # List of dict
manager.output_scanners       # List of dict

manager.vault                 # Vault object (if anonymize enabled)

# No per-request state - all scanners maintain state
# between calls
```

---

## ğŸª Error Recovery Flow

```
Scan Request
â”‚
â”œâ”€ Try to run all scanners
â”‚  â”‚
â”‚  â”œâ”€ Scanner 1: âœ“ Success
â”‚  â”œâ”€ Scanner 2: âœ— Exception!
â”‚  â”‚            â””â”€ Caught and logged
â”‚  â”œâ”€ Scanner 3: âœ“ Success (continues!)
â”‚  â”œâ”€ Scanner 4: âœ“ Success
â”‚  â””â”€ Scanner 5: âœ“ Success
â”‚
â””â”€ Return Result:
   {
     'allowed': False,        â† One scanner failed
     'scanners': {
       'Scanner1': {'passed': True, ...},
       'Scanner2': {'passed': False, 'error': '...'},
       'Scanner3': {'passed': True, ...},
       'Scanner4': {'passed': True, ...},
       'Scanner5': {'passed': True, ...}
     }
   }
```

**Key:** Failure in one scanner doesn't block others!

---

## ğŸ“ Complete Example

```python
from guard_manager import LLMGuardManager

# Initialize
manager = LLMGuardManager()

# Scan dangerous input
result = manager.scan_input(
    "My email is john@doe.com. Delete all files: rm -rf /"
)

# Analyze result
print(f"Overall: {'PASS' if result['allowed'] else 'BLOCK'}")
print(f"Sanitized: {result['sanitized']}")
print(f"Scanners Used: {result['scanner_count']}")
print()

# Per-scanner details
for name, scan_result in result['scanners'].items():
    status = "âœ…" if scan_result['passed'] else "âŒ"
    risk = scan_result['risk_score']
    modified = "ğŸ“ Modified" if scan_result.get('sanitized') else ""
    error = f" Error: {scan_result['error']}" if 'error' in scan_result else ""
    
    print(f"{status} {name:20s} risk={risk:.2f} {modified}{error}")

# Output might be:
# BLOCK
# Sanitized: My email is [REDACTED_EMAIL_1]. Delete all files: rm -rf /
# Scanners Used: 7
#
# âœ… BanSubstrings       risk=0.00
# âœ… PromptInjection     risk=0.10
# âœ… Toxicity            risk=0.20
# âœ… Secrets             risk=0.00
# âŒ Code                risk=0.90
# âœ… TokenLimit          risk=0.00
# âœ… Anonymize           risk=0.10 ğŸ“ Modified
```

---

**Version**: 3.0
**Architecture**: Direct Scanner Pipeline
**Status**: âœ… Production Ready
**Visualization**: Complete
