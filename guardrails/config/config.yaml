# Ollama Guard Proxy Configuration
# YAML configuration file for the Ollama Guard Proxy

# Ollama Backend Configuration
ollama_url: http://192.168.1.2:11434
ollama_path: /api/generate

# Proxy Server Configuration
proxy_host: 0.0.0.0
proxy_port: 8888

# IP Access Control
# Set enable_ip_filter to true to enforce IP-based access control
enable_ip_filter: false

# Whitelist: comma-separated list of allowed IP addresses or CIDR ranges
# Example: "192.168.1.0/24, 10.0.0.1, 172.16.0.0/12"
ip_whitelist: ""

# Blacklist: comma-separated list of blocked IP addresses or CIDR ranges
# Example: "192.168.1.100, 10.0.0.0/8"
ip_blacklist: ""

# LLM Guard Configuration
enable_input_guard: true
enable_output_guard: true

# If true, any error during guard scanning will block the request
# If false, errors are logged but requests are allowed
block_on_guard_error: true

# Local Models Configuration
# Set to true to use locally downloaded models instead of downloading from HuggingFace
use_local_models: false

# Path to directory containing local models
# Models should be organized in subdirectories by model name
models_path: "./models"

# Cache Configuration
cache_enabled: true
cache_backend: "auto"  # Options: 'redis', 'memory', 'auto' (tries Redis, falls back to memory)
cache_ttl: 3600  # Time-to-live in seconds (1 hour)
cache_max_size: 1000  # Max size for in-memory cache

# Redis Configuration (for distributed caching)
redis_enabled: true
redis_host: "localhost"
redis_port: 6379
redis_db: 0
redis_password: null  # Set if Redis requires authentication
redis_max_connections: 50
redis_timeout: 5  # Socket timeout in seconds
redis_key_prefix: "llmguard:"

# Rate Limiting Configuration
rate_limit_enabled: true
rate_limit_per_minute: 60
rate_limit_per_hour: 1000
rate_limit_burst: 10

# Concurrency Configuration (Ollama-style)
# Maximum number of parallel requests each model will process at the same time
# Set to 'auto' to detect based on available memory (4 if >= 16GB, 2 if >= 8GB, else 1)
# Or set a specific number (e.g., 1, 2, 4, 8)
ollama_num_parallel: "auto"

# Maximum number of requests in queue when busy before rejecting
# Default is 512 (same as Ollama)
ollama_max_queue: 512

# Request timeout in seconds (for queued requests)
request_timeout: 300

# Enable per-model queue statistics
enable_queue_stats: true

# Input Scanner Configuration
input_scanners:
  ban_substrings:
    enabled: true
    substrings:
      - "malicious"
      - "dangerous"
  
  prompt_injection:
    enabled: true
  
  toxicity:
    enabled: true
    threshold: 0.5
  
  secrets:
    enabled: true
  
  code:
    enabled: true
  
  token_limit:
    enabled: true
    limit: 4000

# Output Scanner Configuration
output_scanners:
  ban_substrings:
    enabled: true
    substrings:
      - "malicious"
      - "dangerous"
  
  toxicity:
    enabled: true
    threshold: 0.5
  
  malicious_urls:
    enabled: true
  
  no_refusal:
    enabled: true

# Logging Configuration
logging:
  level: INFO
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
