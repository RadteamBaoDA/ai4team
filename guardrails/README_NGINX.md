# Ollama Guard Proxy + Nginx Load Balancer

**Complete solution for deploying Ollama Guard Proxy with Nginx load balancing and IP filtering.**

---

## âš¡ Quick Start (5 minutes)

### Linux/macOS
```bash
cd guardrails
chmod +x deploy-nginx.sh
sudo ./deploy-nginx.sh start
```

### Windows (Admin PowerShell)
```batch
cd guardrails
.\deploy-nginx.bat start
```

### Verify
```bash
curl http://localhost/health
```

---

## ğŸ“š Documentation

| Document | Read Time | Purpose |
|----------|-----------|---------|
| **START_NGINX.md** | 5 min | Visual overview (this file) |
| **NGINX_SETUP_COMPLETE.md** | 30 min | â­ Complete setup guide |
| **NGINX_QUICKREF.md** | 15 min | Quick reference & commands |
| **NGINX_DEPLOYMENT.md** | 60 min | Detailed 12-part guide |
| **NGINX_INDEX.md** | 10 min | Navigation & file index |

---

## ğŸ¯ What This Provides

### Load Balancing
- Distributes requests across 3 proxy instances (8080, 8081, 8082)
- Least connections algorithm
- Automatic failover
- Health monitoring

### IP Filtering  
- Whitelist specific IPs (allow only these)
- Blacklist specific IPs (deny these)
- CIDR notation support (192.168.1.0/24)
- IPv6 ready

### Rate Limiting
- 10 requests/second per IP (configurable)
- Burst support
- Different limits for different endpoints
- 429 response on limit exceeded

### Security
- SSL/TLS encryption
- Security headers
- Input/output scanning
- Audit logging

---

## ğŸ“‚ Key Files

```
guardrails/
â”œâ”€â”€ deploy-nginx.sh                (Linux/macOS auto-deployment)
â”œâ”€â”€ deploy-nginx.bat               (Windows auto-deployment)
â”œâ”€â”€ nginx-ollama-production.conf   (Nginx configuration)
â”œâ”€â”€ run_proxy.sh                   (Proxy runner - Linux/macOS)
â”œâ”€â”€ run_proxy.bat                  (Proxy runner - Windows)
â”œâ”€â”€ ollama_guard_proxy.py          (Main proxy app)
â””â”€â”€ NGINX_*.md                     (Documentation)
```

---

## ğŸš€ Deployment

### Automated (Recommended)
```bash
sudo ./deploy-nginx.sh start    # Start all services
sudo ./deploy-nginx.sh status   # Check status
sudo ./deploy-nginx.sh stop     # Stop all services
```

### Manual
```bash
# 1. Start proxies
./run_proxy.sh --port 8080 --workers 4 &
./run_proxy.sh --port 8081 --workers 4 &
./run_proxy.sh --port 8082 --workers 4 &

# 2. Configure Nginx
sudo cp nginx-ollama-production.conf /etc/nginx/sites-available/ollama-guard
sudo ln -s /etc/nginx/sites-available/ollama-guard /etc/nginx/sites-enabled/

# 3. Start Nginx
sudo systemctl start nginx

# 4. Verify
curl http://localhost/health
```

---

## ğŸ§ª Testing

### Health Check
```bash
curl http://localhost/health | jq
```

### Backend Status
```bash
curl http://localhost:8080/health
curl http://localhost:8081/health
curl http://localhost:8082/health
```

### API Call
```bash
curl -X POST http://localhost/v1/generate \
  -H "Content-Type: application/json" \
  -d '{"model":"mistral","prompt":"Hello","stream":false}'
```

### Load Test
```bash
for i in {1..10}; do
  curl -s http://localhost/health &
done
wait
```

---

## ğŸ“Š Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Internet Clients        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚ (Port 80/443)
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Nginx          â”‚
    â”‚  - Load Balance â”‚
    â”‚  - IP Filter    â”‚
    â”‚  - Rate Limit   â”‚
    â”‚  - SSL/TLS      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”
    â–¼        â–¼        â–¼
  Proxy1   Proxy2   Proxy3
  :8080    :8081    :8082
  4 Work   4 Work   4 Work
  128 Con  128 Con  128 Con
    â”‚        â”‚        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Ollama Backend â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ Configuration

### Enable IP Whitelist
Edit `nginx-ollama-production.conf`:
```nginx
geo $ip_allowed {
    default 0;
    192.168.1.0/24 1;   # Your office
    10.0.0.0/8 1;       # Your VPN
}

# In server block, uncomment:
if ($ip_allowed = 0) {
    return 403;
}
```

### Adjust Rate Limit
```nginx
# Change from 10r/s to 20r/s
limit_req_zone $binary_remote_addr zone=api_limit:10m rate=20r/s;
```

### Add More Proxies
```bash
./run_proxy.sh --port 8083 --workers 8 &

# Add to nginx.conf:
# server 127.0.0.1:8083;

sudo nginx -s reload
```

---

## ğŸ“ˆ Performance

### Default Setup
- Capacity: ~1,500 concurrent connections
- Throughput: Limited by Ollama backend
- Response time: 150-700ms (with guards)
- Deployment time: 5 minutes

### Scaling
| Load | Proxies | Workers | Config |
|------|---------|---------|--------|
| Light | 3 | 2 | dev |
| Medium | 3 | 4 | default |
| Heavy | 3-5 | 8 | production |

---

## ğŸ” Monitoring

### Status
```bash
./deploy-nginx.sh status
```

### Logs (Real-time)
```bash
tail -f /var/log/nginx/ollama_guard_access.log
tail -f /var/log/nginx/ollama_guard_error.log
```

### Request Distribution
```bash
sudo tail -1000 /var/log/nginx/ollama_guard_access.log | \
  awk '{print $7}' | sort | uniq -c
```

---

## ğŸ› Troubleshooting

### Port Already in Use
```bash
lsof -i :80
lsof -i :8080
kill -9 <PID>
```

### Nginx Won't Start
```bash
sudo nginx -t          # Test config
tail -20 /var/log/nginx/error.log
```

### Proxies Not Responding
```bash
curl http://localhost:8080/health
curl http://localhost:8081/health
curl http://localhost:8082/health
```

### High Error Rate
```bash
tail -100 /var/log/nginx/error.log
./deploy-nginx.sh test
```

See `TROUBLESHOOTING.md` for more issues.

---

## ğŸ” Security

### Implemented
- âœ… SSL/TLS encryption
- âœ… IP access control
- âœ… Rate limiting
- âœ… Security headers
- âœ… Audit logging

### Best Practices
- Use Let's Encrypt for SSL
- Configure IP whitelist
- Monitor logs regularly
- Keep systems updated
- Regular audits

---

## ğŸ“‹ Pre-deployment Checklist

- [ ] Prerequisites: Python 3.9+, Nginx, Uvicorn
- [ ] Ollama backend running and accessible
- [ ] Ports available: 80, 443, 8080, 8081, 8082
- [ ] Sufficient disk space for logs
- [ ] Network connectivity verified
- [ ] SSL certificates ready (or use self-signed)

---

## âœ… Post-deployment Checklist

- [ ] `./deploy-nginx.sh status` shows all running
- [ ] `curl http://localhost/health` returns 200
- [ ] Backend ports 8080/8081/8082 responding
- [ ] Health endpoint accessible through Nginx
- [ ] Logs being written to `/var/log/nginx/`
- [ ] IP filtering tested (if configured)
- [ ] Rate limiting tested
- [ ] Load test completed
- [ ] Monitoring setup done

---

## ğŸ¯ Next Steps

1. **Immediate** (5 min)
   - Run: `sudo ./deploy-nginx.sh start`
   - Test: `curl http://localhost/health`

2. **Short-term** (30 min)
   - Read: `NGINX_SETUP_COMPLETE.md`
   - Configure IP filtering (if needed)
   - Run: `./deploy-nginx.sh test`

3. **Long-term**
   - Monitor logs regularly
   - Adjust workers for your load
   - Plan for scaling
   - Maintain security updates

---

## ğŸ“ Support

### Documentation
- `NGINX_SETUP_COMPLETE.md` - Complete guide
- `NGINX_QUICKREF.md` - Quick reference
- `NGINX_DEPLOYMENT.md` - Detailed guide
- `TROUBLESHOOTING.md` - Problem solving

### Commands
```bash
./deploy-nginx.sh test       # Run diagnostics
./deploy-nginx.sh status     # Check status
./deploy-nginx.sh restart    # Restart all

curl http://localhost/health # Health check
tail -f /var/log/nginx/ollama_guard_access.log  # View logs
```

---

## ğŸŠ Summary

You have everything needed to:
- âœ… Deploy Ollama Guard Proxy with Nginx
- âœ… Load balance across multiple instances
- âœ… Filter requests by IP (whitelist/blacklist)
- âœ… Rate limit per IP
- âœ… Monitor and troubleshoot
- âœ… Scale horizontally

**Start now**: `sudo ./deploy-nginx.sh start`

---

**Version**: 1.0  
**Status**: Production Ready  
**Deployment Time**: 5 minutes  
**Support**: Full documentation included
